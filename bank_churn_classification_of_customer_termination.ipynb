{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports and Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, f1_score, precision_recall_curve, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upsample_func(features, target, repeat):\n",
    "    features_zeros = features[target == 0]\n",
    "    features_ones = features[target == 1]\n",
    "    target_zeros = target[target == 0]\n",
    "    target_ones = target[target == 1]\n",
    "\n",
    "    features_upsampled = pd.concat([features_zeros] + [features_ones] * repeat)\n",
    "    target_upsampled = pd.concat([target_zeros] + [target_ones] * repeat)\n",
    "    \n",
    "    features_upsampled, target_upsampled = shuffle(\n",
    "        features_upsampled, target_upsampled, random_state=12345)\n",
    "    \n",
    "    return features_upsampled, target_upsampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [],
   "source": [
    "def downsample_func(features, target, fraction):\n",
    "    features_zeros = features[target == 0]\n",
    "    features_ones = features[target == 1]\n",
    "    target_zeros = target[target == 0]\n",
    "    target_ones = target[target == 1]\n",
    "\n",
    "    features_downsampled = pd.concat(\n",
    "        [features_zeros.sample(frac=fraction, random_state=12345)] + [features_ones])\n",
    "    target_downsampled = pd.concat(\n",
    "        [target_zeros.sample(frac=fraction, random_state=12345)] + [target_ones])\n",
    "    \n",
    "    features_downsampled, target_downsampled = shuffle(\n",
    "        features_downsampled, target_downsampled, random_state=12345)\n",
    "    \n",
    "    return features_downsampled, target_downsampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validate(validator, model, features, target, score=['f1','AUC_ROC','confusion'], \\\n",
    "                   scaler=None ,scale=False,scale_col=None, upsample=False, downsample=False, sample_ratio=None):\n",
    "    #save all scores in df\n",
    "    scores = pd.DataFrame(index=range(0,validator.n_splits), columns=score)\n",
    "#     scores = {}\n",
    "#     for s in score:\n",
    "#         scores[s] = []\n",
    "    #save sanity checks in df\n",
    "    sanity_check = pd.DataFrame(index=range(0,validator.n_splits), columns=['predicted 1s ratio','f1 constant 1s'])\n",
    "    fold=0\n",
    "    #iterate over splits\n",
    "    for train_index, valid_index in validator.split(features, target):\n",
    "        #copying sets to perform scales / sampling\n",
    "        features_train_edit = features.loc[train_index,:].copy()\n",
    "        target_train_edit = target[train_index].copy()\n",
    "        features_valid_edit = features.loc[valid_index,:].copy()\n",
    "        target_valid_edit = target[valid_index].copy()\n",
    "#        print(target_train_edit.mean(), target_valid_edit.mean())\n",
    "        #scale data\n",
    "        if scale and scaler is not None and scale_col is not None:\n",
    "            scaler.fit(features_train_edit[scale_col])\n",
    "            features_train_edit.loc[:,scale_col] = scaler.transform(features_train_edit[scale_col])\n",
    "            features_valid_edit.loc[:,scale_col] = scaler.transform(features_valid_edit[scale_col])\n",
    "        #upsample\n",
    "        if upsample and sample_ratio is not None:\n",
    "            features_train_edit, target_train_edit = upsample_func(features_train_edit, target_train_edit, sample_ratio)\n",
    "        #downsample\n",
    "        if downsample and sample_ratio is not None:\n",
    "            features_train_edit, target_train_edit = downsample_func(features_train_edit, target_train_edit, 1/sample_ratio)\n",
    "        #train model\n",
    "        model.fit(features_train_edit,target_train_edit)\n",
    "        #evaluate predictions\n",
    "        predicted_valid = model.predict(features_valid_edit)\n",
    "        if 'f1' in score:\n",
    "            scores.loc[fold,'f1'] = (round(f1_score(target_valid_edit, predicted_valid),5))\n",
    "        if 'AUC_ROC' in score:\n",
    "            probabilities_valid = model.predict_proba(features_valid_edit)\n",
    "            probabilities_one_valid = probabilities_valid[:, 1]\n",
    "            scores.loc[fold,'AUC_ROC'] = (round(roc_auc_score(target_valid_edit, probabilities_one_valid),5))\n",
    "        if 'confusion' in score:\n",
    "            scores.loc[fold,'confusion'] = (confusion_matrix(target_valid_edit, predicted_valid))\n",
    "        #perform sanity checks\n",
    "        #1's ratio in predicted data\n",
    "        sanity_check.loc[fold,'predicted 1s ratio'] = predicted_valid.mean()\n",
    "        #f1 score for a constant model with all 1's\n",
    "        sanity_check.loc[fold,'f1 constant 1s'] = f1_score(target_valid_edit,(pd.Series(1,target_valid_edit.index)))\n",
    "        fold+=1\n",
    "    return scores, sanity_check\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10000 entries, 0 to 9999\n",
      "Data columns (total 14 columns):\n",
      "RowNumber          10000 non-null int64\n",
      "CustomerId         10000 non-null int64\n",
      "Surname            10000 non-null object\n",
      "CreditScore        10000 non-null int64\n",
      "Geography          10000 non-null object\n",
      "Gender             10000 non-null object\n",
      "Age                10000 non-null int64\n",
      "Tenure             9091 non-null float64\n",
      "Balance            10000 non-null float64\n",
      "NumOfProducts      10000 non-null int64\n",
      "HasCrCard          10000 non-null int64\n",
      "IsActiveMember     10000 non-null int64\n",
      "EstimatedSalary    10000 non-null float64\n",
      "Exited             10000 non-null int64\n",
      "dtypes: float64(3), int64(8), object(3)\n",
      "memory usage: 1.1+ MB\n"
     ]
    }
   ],
   "source": [
    "#read file\n",
    "data = pd.read_csv('/datasets/Churn.csv')\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2037"
      ]
     },
     "execution_count": 359,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#1's ratio\n",
    "data['Exited'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling textual features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['France', 'Spain', 'Germany'], dtype=object)"
      ]
     },
     "execution_count": 360,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#examining textual features and their importance\n",
    "\n",
    "#data['Surname'].head()\n",
    "data['Geography'].head()\n",
    "data['Geography'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Female', 'Male'], dtype=object)"
      ]
     },
     "execution_count": 361,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['Gender'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 3 textual features:\n",
    "- 'Surname' we can drop since it's not categorical and probably of less importance\n",
    "- 'Geography' and 'Gender' have only 2-3 categories - it's best to One-Hot encode them for both Logistic Regression and Random Forest use\n",
    "\n",
    "In addition, columns 'RowNumber' and 'CustomerId' should be dropped since they are meaningless for prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dropping meaningless columns\n",
    "data_prep = data.drop(['RowNumber', 'CustomerId', 'Surname'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10000 entries, 0 to 9999\n",
      "Data columns (total 14 columns):\n",
      "CreditScore          10000 non-null int64\n",
      "Age                  10000 non-null int64\n",
      "Tenure               9091 non-null float64\n",
      "Balance              10000 non-null float64\n",
      "NumOfProducts        10000 non-null int64\n",
      "HasCrCard            10000 non-null int64\n",
      "IsActiveMember       10000 non-null int64\n",
      "EstimatedSalary      10000 non-null float64\n",
      "Exited               10000 non-null int64\n",
      "Geography_France     10000 non-null uint8\n",
      "Geography_Germany    10000 non-null uint8\n",
      "Geography_Spain      10000 non-null uint8\n",
      "Gender_Female        10000 non-null uint8\n",
      "Gender_Male          10000 non-null uint8\n",
      "dtypes: float64(3), int64(6), uint8(5)\n",
      "memory usage: 752.1 KB\n"
     ]
    }
   ],
   "source": [
    "#One-Hot encoding of Geography and Gender\n",
    "data_prep = pd.get_dummies(data_prep)\n",
    "data_prep.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CreditScore         -0.000062\n",
       "Age                 -0.013134\n",
       "Tenure               1.000000\n",
       "Balance             -0.007911\n",
       "NumOfProducts        0.011979\n",
       "HasCrCard            0.027232\n",
       "IsActiveMember      -0.032178\n",
       "EstimatedSalary      0.010520\n",
       "Exited              -0.016761\n",
       "Geography_France     0.002167\n",
       "Geography_Germany   -0.003299\n",
       "Geography_Spain      0.000810\n",
       "Gender_Female       -0.012634\n",
       "Gender_Male          0.012634\n",
       "Name: Tenure, dtype: float64"
      ]
     },
     "execution_count": 364,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_prep.corr()['Tenure']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'Tenure' is not correlated with any other column so we can't recover it by another"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    9091.000000\n",
       "mean        4.997690\n",
       "std         2.894723\n",
       "min         0.000000\n",
       "25%         2.000000\n",
       "50%         5.000000\n",
       "75%         7.000000\n",
       "max        10.000000\n",
       "Name: Tenure, dtype: float64"
      ]
     },
     "execution_count": 365,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#viewing Tenure statistics\n",
    "data_prep.describe()['Tenure']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since missing data is less than 10% it's ok to fill with median for training purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10000 entries, 0 to 9999\n",
      "Data columns (total 14 columns):\n",
      "CreditScore          10000 non-null int64\n",
      "Age                  10000 non-null int64\n",
      "Tenure               10000 non-null float64\n",
      "Balance              10000 non-null float64\n",
      "NumOfProducts        10000 non-null int64\n",
      "HasCrCard            10000 non-null int64\n",
      "IsActiveMember       10000 non-null int64\n",
      "EstimatedSalary      10000 non-null float64\n",
      "Exited               10000 non-null int64\n",
      "Geography_France     10000 non-null uint8\n",
      "Geography_Germany    10000 non-null uint8\n",
      "Geography_Spain      10000 non-null uint8\n",
      "Gender_Female        10000 non-null uint8\n",
      "Gender_Male          10000 non-null uint8\n",
      "dtypes: float64(3), int64(6), uint8(5)\n",
      "memory usage: 752.1 KB\n"
     ]
    }
   ],
   "source": [
    "#filling missing Tenure\n",
    "data_prep['Tenure'] = data_prep['Tenure'].fillna(data_prep['Tenure'].median())\n",
    "data_prep.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting to cross-validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train 1s:  0.203125 test 1s:  0.206\n"
     ]
    }
   ],
   "source": [
    "#splitting with 20% test \n",
    "features_train, features_test, target_train, target_test = \\\n",
    "    train_test_split(data_prep.drop(['Exited'] , axis=1), data_prep['Exited'], \\\n",
    "                     test_size=0.2, random_state=1234)\n",
    "features_train.reset_index(drop=True, inplace=True)\n",
    "target_train.reset_index(drop=True, inplace=True)\n",
    "#checking balance\n",
    "print('train 1s: ', target_train.mean(), 'test 1s: ',target_test.mean() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [],
   "source": [
    "#StratifiedKFold cross-validator\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=12345)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [],
   "source": [
    "#columns to scale\n",
    "numeric = ['CreditScore', 'Age', 'Tenure', 'Balance', 'NumOfProducts', 'EstimatedSalary']\n",
    "\n",
    "#tunning scaler on train set\n",
    "scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training model without balancing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2037"
      ]
     },
     "execution_count": 373,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#examining imbalace\n",
    "data_prep['Exited'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "only 20% of data is class '1'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training with logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression(solver='liblinear', random_state = 12345)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores, sanity_check = cross_validate(skf,  model, features_train, target_train, \\\n",
    "                                      score=['f1','AUC_ROC','confusion'], \\\n",
    "                        scaler=scaler,scale=True,scale_col=numeric, \\\n",
    "                                      upsample=False, downsample=False, sample_ratio=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>f1</th>\n",
       "      <th>AUC_ROC</th>\n",
       "      <th>confusion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.33257</td>\n",
       "      <td>0.76156</td>\n",
       "      <td>[[1234, 41], [252, 73]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.33411</td>\n",
       "      <td>0.80181</td>\n",
       "      <td>[[1241, 34], [253, 72]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.31081</td>\n",
       "      <td>0.78978</td>\n",
       "      <td>[[1225, 50], [256, 69]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.30131</td>\n",
       "      <td>0.73949</td>\n",
       "      <td>[[1211, 64], [256, 69]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.28306</td>\n",
       "      <td>0.74238</td>\n",
       "      <td>[[1230, 45], [264, 61]]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        f1  AUC_ROC                confusion\n",
       "0  0.33257  0.76156  [[1234, 41], [252, 73]]\n",
       "1  0.33411  0.80181  [[1241, 34], [253, 72]]\n",
       "2  0.31081  0.78978  [[1225, 50], [256, 69]]\n",
       "3  0.30131  0.73949  [[1211, 64], [256, 69]]\n",
       "4  0.28306  0.74238  [[1230, 45], [264, 61]]"
      ]
     },
     "execution_count": 376,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>f1</th>\n",
       "      <th>AUC_ROC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>count</td>\n",
       "      <td>5.000</td>\n",
       "      <td>5.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>mean</td>\n",
       "      <td>0.312</td>\n",
       "      <td>0.767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>std</td>\n",
       "      <td>0.022</td>\n",
       "      <td>0.028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>min</td>\n",
       "      <td>0.283</td>\n",
       "      <td>0.739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50%</td>\n",
       "      <td>0.311</td>\n",
       "      <td>0.762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>max</td>\n",
       "      <td>0.334</td>\n",
       "      <td>0.802</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          f1  AUC_ROC\n",
       "count  5.000    5.000\n",
       "mean   0.312    0.767\n",
       "std    0.022    0.028\n",
       "min    0.283    0.739\n",
       "50%    0.311    0.762\n",
       "max    0.334    0.802"
      ]
     },
     "execution_count": 377,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores[['f1','AUC_ROC']].astype('float64').describe(percentiles=[]).round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>predicted 1s ratio</th>\n",
       "      <th>f1 constant 1s</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.07125</td>\n",
       "      <td>0.337662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.06625</td>\n",
       "      <td>0.337662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.074375</td>\n",
       "      <td>0.337662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.083125</td>\n",
       "      <td>0.337662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.06625</td>\n",
       "      <td>0.337662</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  predicted 1s ratio f1 constant 1s\n",
       "0            0.07125       0.337662\n",
       "1            0.06625       0.337662\n",
       "2           0.074375       0.337662\n",
       "3           0.083125       0.337662\n",
       "4            0.06625       0.337662"
      ]
     },
     "execution_count": 378,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sanity_check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The data is strongly skewed with only 20% '1's \n",
    "- predicted data has less than 10% 1's compared with 20% expected\n",
    "- F1 score is low (worst case 0.28), lower than F1 score of a constant model with all 1's (0.337)\n",
    "- AUC ROC score is moderate 0.73\n",
    "- Since there is a strong imbalance we can't count on ROC AUC - it gave a moderately high graph because FPR is lowered due to large N and model guessing much more 0's (low FP), as can be seen in confusion matrix - FN is 3 times larger than TP. So the model actually didn't achieve a good TPR.\n",
    "\n",
    "This model is not very good."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training with random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {},
   "outputs": [],
   "source": [
    "#searching for best n to maximize f1, AUC ROC\n",
    "\n",
    "scores_dict = {} \n",
    "sanity_check_dict = {}\n",
    "\n",
    "for n_estimators in range(8,50):\n",
    "    model = RandomForestClassifier(random_state=12340+n_estimators, n_estimators=n_estimators)\n",
    "    \n",
    "    scores_dict[n_estimators], sanity_check_dict[n_estimators] = \\\n",
    "        cross_validate(skf,  model, features_train, target_train, \\\n",
    "                                          score=['f1','AUC_ROC','confusion'], \\\n",
    "                            scaler=scaler,scale=True,scale_col=numeric, \\\n",
    "                                          upsample=False, downsample=False, sample_ratio=None)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max F1:  0.57736 n_estimators:  47 AUC ROC:  0.82461 1's ratio:  0.120625\n"
     ]
    }
   ],
   "source": [
    "#N that maximize F1\n",
    "max_N = 8\n",
    "for N in scores_dict.keys():\n",
    "    if scores_dict[max_N]['f1'].min() < scores_dict[N]['f1'].min():\n",
    "        max_N = N        \n",
    "print('max F1: ', scores_dict[max_N]['f1'].min(),'n_estimators: ', max_N, \\\n",
    "      'AUC ROC: ',scores_dict[max_N]['AUC_ROC'].min(), \\\n",
    "      '1\\'s ratio: ', sanity_check_dict[max_N]['predicted 1s ratio'].min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>f1</th>\n",
       "      <th>AUC_ROC</th>\n",
       "      <th>confusion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.58974</td>\n",
       "      <td>0.85088</td>\n",
       "      <td>[[1215, 60], [164, 161]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.59459</td>\n",
       "      <td>0.8772</td>\n",
       "      <td>[[1236, 39], [171, 154]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.58301</td>\n",
       "      <td>0.85367</td>\n",
       "      <td>[[1233, 42], [174, 151]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.57736</td>\n",
       "      <td>0.83791</td>\n",
       "      <td>[[1223, 52], [172, 153]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.57786</td>\n",
       "      <td>0.82461</td>\n",
       "      <td>[[1221, 54], [171, 154]]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        f1  AUC_ROC                 confusion\n",
       "0  0.58974  0.85088  [[1215, 60], [164, 161]]\n",
       "1  0.59459   0.8772  [[1236, 39], [171, 154]]\n",
       "2  0.58301  0.85367  [[1233, 42], [174, 151]]\n",
       "3  0.57736  0.83791  [[1223, 52], [172, 153]]\n",
       "4  0.57786  0.82461  [[1221, 54], [171, 154]]"
      ]
     },
     "execution_count": 392,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores_dict[47]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- f1 score improves significanly to 0.577 (worst case) with Random Forest model with N=47 (greedy)\n",
    "- AUC ROC score improves to 0.824\n",
    "- Only 12% predictions of 1's, the same as in Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training model with balancing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training with Weight Adjustment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training with logistic resression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression(solver='liblinear', random_state = 12345, class_weight='balanced')\n",
    "# model.fit(features_train, target_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores, sanity_check = cross_validate(skf,  model, features_train, target_train, \\\n",
    "                                      score=['f1','AUC_ROC','confusion'], \\\n",
    "                        scaler=scaler,scale=True,scale_col=numeric, \\\n",
    "                                      upsample=False, downsample=False, sample_ratio=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>f1</th>\n",
       "      <th>AUC_ROC</th>\n",
       "      <th>confusion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.48939</td>\n",
       "      <td>0.76374</td>\n",
       "      <td>[[924, 351], [106, 219]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.5271</td>\n",
       "      <td>0.80531</td>\n",
       "      <td>[[907, 368], [77, 248]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.51173</td>\n",
       "      <td>0.79267</td>\n",
       "      <td>[[934, 341], [96, 229]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.47414</td>\n",
       "      <td>0.74435</td>\n",
       "      <td>[[892, 383], [105, 220]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.46053</td>\n",
       "      <td>0.74585</td>\n",
       "      <td>[[898, 377], [115, 210]]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        f1  AUC_ROC                 confusion\n",
       "0  0.48939  0.76374  [[924, 351], [106, 219]]\n",
       "1   0.5271  0.80531   [[907, 368], [77, 248]]\n",
       "2  0.51173  0.79267   [[934, 341], [96, 229]]\n",
       "3  0.47414  0.74435  [[892, 383], [105, 220]]\n",
       "4  0.46053  0.74585  [[898, 377], [115, 210]]"
      ]
     },
     "execution_count": 396,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>f1</th>\n",
       "      <th>AUC_ROC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>count</td>\n",
       "      <td>5.000</td>\n",
       "      <td>5.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>mean</td>\n",
       "      <td>0.493</td>\n",
       "      <td>0.770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>std</td>\n",
       "      <td>0.027</td>\n",
       "      <td>0.028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>min</td>\n",
       "      <td>0.461</td>\n",
       "      <td>0.744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50%</td>\n",
       "      <td>0.489</td>\n",
       "      <td>0.764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>max</td>\n",
       "      <td>0.527</td>\n",
       "      <td>0.805</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          f1  AUC_ROC\n",
       "count  5.000    5.000\n",
       "mean   0.493    0.770\n",
       "std    0.027    0.028\n",
       "min    0.461    0.744\n",
       "50%    0.489    0.764\n",
       "max    0.527    0.805"
      ]
     },
     "execution_count": 397,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores[['f1','AUC_ROC']].astype('float64').describe(percentiles=[]).round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>predicted 1s ratio</th>\n",
       "      <th>f1 constant 1s</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.35625</td>\n",
       "      <td>0.337662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.385</td>\n",
       "      <td>0.337662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.35625</td>\n",
       "      <td>0.337662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.376875</td>\n",
       "      <td>0.337662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.366875</td>\n",
       "      <td>0.337662</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  predicted 1s ratio f1 constant 1s\n",
       "0            0.35625       0.337662\n",
       "1              0.385       0.337662\n",
       "2            0.35625       0.337662\n",
       "3           0.376875       0.337662\n",
       "4           0.366875       0.337662"
      ]
     },
     "execution_count": 398,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sanity_check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- f1 score improves to 0.46 compared with logistic resression with no balancing (0.28) but is lower than random forest with no balancing (0.56)\n",
    "- AUC ROC still lower than in random forest\n",
    "- 37% predictions of 1's when there are 20% in data\n",
    "- TN decreases, TP increases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training with random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {},
   "outputs": [],
   "source": [
    "#searching for best n to maximize f1, AUC ROC\n",
    "\n",
    "scores_dict = {} \n",
    "sanity_check_dict = {}\n",
    "\n",
    "for n_estimators in range(8,50):\n",
    "    model = RandomForestClassifier(random_state=12340+n_estimators, n_estimators=n_estimators, class_weight='balanced')\n",
    "    \n",
    "    scores_dict[n_estimators], sanity_check_dict[n_estimators] = \\\n",
    "        cross_validate(skf,  model, features_train, target_train, \\\n",
    "                                          score=['f1','AUC_ROC','confusion'], \\\n",
    "                            scaler=scaler,scale=True,scale_col=numeric, \\\n",
    "                                          upsample=False, downsample=False, sample_ratio=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max F1:  0.55133 n_estimators:  36 AUC ROC:  0.83135 1's ratio:  0.10625\n"
     ]
    }
   ],
   "source": [
    "#N that maximize F1\n",
    "max_N = 8\n",
    "for N in scores_dict.keys():\n",
    "    if scores_dict[max_N]['f1'].min() < scores_dict[N]['f1'].min():\n",
    "        max_N = N        \n",
    "print('max F1: ', scores_dict[max_N]['f1'].min(),'n_estimators: ', max_N, \\\n",
    "      'AUC ROC: ',scores_dict[max_N]['AUC_ROC'].min(), \\\n",
    "      '1\\'s ratio: ', sanity_check_dict[max_N]['predicted 1s ratio'].min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>f1</th>\n",
       "      <th>AUC_ROC</th>\n",
       "      <th>confusion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.55877</td>\n",
       "      <td>0.85273</td>\n",
       "      <td>[[1226, 49], [180, 145]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.57874</td>\n",
       "      <td>0.86807</td>\n",
       "      <td>[[1239, 36], [178, 147]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.55758</td>\n",
       "      <td>0.85886</td>\n",
       "      <td>[[1243, 32], [187, 138]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.56699</td>\n",
       "      <td>0.83601</td>\n",
       "      <td>[[1231, 44], [179, 146]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.55133</td>\n",
       "      <td>0.83135</td>\n",
       "      <td>[[1219, 56], [180, 145]]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        f1  AUC_ROC                 confusion\n",
       "0  0.55877  0.85273  [[1226, 49], [180, 145]]\n",
       "1  0.57874  0.86807  [[1239, 36], [178, 147]]\n",
       "2  0.55758  0.85886  [[1243, 32], [187, 138]]\n",
       "3  0.56699  0.83601  [[1231, 44], [179, 146]]\n",
       "4  0.55133  0.83135  [[1219, 56], [180, 145]]"
      ]
     },
     "execution_count": 405,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores_dict[36]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- f1 score is lower than in random forest with no balancing\n",
    "- AUC ROC is a bit larger\n",
    "- 10% predictions of 1's when there are 20% in data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Balancing with Weight Adjustment didn't improve the models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training with upsampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.923076923076923"
      ]
     },
     "execution_count": 408,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#smapling ratio\n",
    "(1-target_train.mean())/target_train.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training with random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {},
   "outputs": [],
   "source": [
    "#searching for best n to maximize f1, AUC ROC\n",
    "\n",
    "scores_dict = {} \n",
    "sanity_check_dict = {}\n",
    "\n",
    "for n_estimators in range(8,50):\n",
    "    model = RandomForestClassifier(random_state=12340+n_estimators, n_estimators=n_estimators)\n",
    "    \n",
    "    scores_dict[n_estimators], sanity_check_dict[n_estimators] = \\\n",
    "        cross_validate(skf,  model, features_train, target_train, \\\n",
    "                                          score=['f1','AUC_ROC','confusion'], \\\n",
    "                            scaler=scaler,scale=True,scale_col=numeric, \\\n",
    "                                          upsample=True, downsample=False, sample_ratio=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max F1:  0.59022 n_estimators:  41 AUC ROC:  0.82474 1's ratio:  0.1575\n"
     ]
    }
   ],
   "source": [
    "#N that maximize F1\n",
    "max_N = 8\n",
    "for N in scores_dict.keys():\n",
    "    if scores_dict[max_N]['f1'].min() < scores_dict[N]['f1'].min():\n",
    "        max_N = N        \n",
    "print('max F1: ', scores_dict[max_N]['f1'].min(),'n_estimators: ', max_N, \\\n",
    "      'AUC ROC: ',scores_dict[max_N]['AUC_ROC'].min(), \\\n",
    "      '1\\'s ratio: ', sanity_check_dict[max_N]['predicted 1s ratio'].min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>f1</th>\n",
       "      <th>AUC_ROC</th>\n",
       "      <th>confusion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.60496</td>\n",
       "      <td>0.84185</td>\n",
       "      <td>[[1178, 97], [142, 183]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.6136</td>\n",
       "      <td>0.869</td>\n",
       "      <td>[[1182, 93], [140, 185]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.60312</td>\n",
       "      <td>0.85273</td>\n",
       "      <td>[[1197, 78], [151, 174]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.59022</td>\n",
       "      <td>0.83721</td>\n",
       "      <td>[[1182, 93], [150, 175]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.5906</td>\n",
       "      <td>0.82474</td>\n",
       "      <td>[[1180, 95], [149, 176]]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        f1  AUC_ROC                 confusion\n",
       "0  0.60496  0.84185  [[1178, 97], [142, 183]]\n",
       "1   0.6136    0.869  [[1182, 93], [140, 185]]\n",
       "2  0.60312  0.85273  [[1197, 78], [151, 174]]\n",
       "3  0.59022  0.83721  [[1182, 93], [150, 175]]\n",
       "4   0.5906  0.82474  [[1180, 95], [149, 176]]"
      ]
     },
     "execution_count": 412,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores_dict[41]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- f1 score improves a little to 0.59 comapred with 0.577 in random forest with no balancing\n",
    "- AUC ROC is the same\n",
    "- 15% predictions of 1's, an improvement compared with 12% with no balancing\n",
    "- TN decreases, TP and FP increases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training with downsampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training with random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {},
   "outputs": [],
   "source": [
    "#searching for best n to maximize f1, AUC ROC\n",
    "\n",
    "scores_dict = {} \n",
    "sanity_check_dict = {}\n",
    "\n",
    "for n_estimators in range(8,50):\n",
    "    model = RandomForestClassifier(random_state=12340+n_estimators, n_estimators=n_estimators)\n",
    "    \n",
    "    scores_dict[n_estimators], sanity_check_dict[n_estimators] = \\\n",
    "        cross_validate(skf,  model, features_train, target_train, \\\n",
    "                                          score=['f1','AUC_ROC','confusion'], \\\n",
    "                            scaler=scaler,scale=True,scale_col=numeric, \\\n",
    "                                          upsample=False, downsample=True, sample_ratio=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max F1:  0.56912 n_estimators:  49 AUC ROC:  0.83414 1's ratio:  0.319375\n"
     ]
    }
   ],
   "source": [
    "#N that maximize F1\n",
    "max_N = 8\n",
    "for N in scores_dict.keys():\n",
    "    if scores_dict[max_N]['f1'].min() < scores_dict[N]['f1'].min():\n",
    "        max_N = N        \n",
    "print('max F1: ', scores_dict[max_N]['f1'].min(),'n_estimators: ', max_N, \\\n",
    "      'AUC ROC: ',scores_dict[max_N]['AUC_ROC'].min(), \\\n",
    "      '1\\'s ratio: ', sanity_check_dict[max_N]['predicted 1s ratio'].min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>f1</th>\n",
       "      <th>AUC_ROC</th>\n",
       "      <th>confusion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.57809</td>\n",
       "      <td>0.83893</td>\n",
       "      <td>[[990, 285], [77, 248]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.6071</td>\n",
       "      <td>0.87259</td>\n",
       "      <td>[[992, 283], [60, 265]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.57279</td>\n",
       "      <td>0.85605</td>\n",
       "      <td>[[1002, 273], [85, 240]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.56912</td>\n",
       "      <td>0.83414</td>\n",
       "      <td>[[979, 296], [78, 247]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.57416</td>\n",
       "      <td>0.8387</td>\n",
       "      <td>[[1004, 271], [85, 240]]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        f1  AUC_ROC                 confusion\n",
       "0  0.57809  0.83893   [[990, 285], [77, 248]]\n",
       "1   0.6071  0.87259   [[992, 283], [60, 265]]\n",
       "2  0.57279  0.85605  [[1002, 273], [85, 240]]\n",
       "3  0.56912  0.83414   [[979, 296], [78, 247]]\n",
       "4  0.57416   0.8387  [[1004, 271], [85, 240]]"
      ]
     },
     "execution_count": 417,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores_dict[49]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- f1 score is lower random forest with upsampling\n",
    "- AUC ROC is higher\n",
    "- 31% predictions of 1's"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training with Random Forest N=41 with upsmapling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = features_train.copy()\n",
    "target = target_train.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StandardScaler(copy=True, with_mean=True, with_std=True)"
      ]
     },
     "execution_count": 419,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#fit scaler on all training set\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(features[numeric])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scale training set\n",
    "features.loc[:,numeric] = scaler.transform(features[numeric])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "metadata": {},
   "outputs": [],
   "source": [
    "#upsample\n",
    "features_upsampled, target_upsampled = upsample(features, target, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #downsample\n",
    "# features_downsampled, target_downsampled = downsample_func(features, target, 1/4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5048543689320388"
      ]
     },
     "execution_count": 423,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_upsampled.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "                       max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, n_estimators=41,\n",
       "                       n_jobs=None, oob_score=False, random_state=12381,\n",
       "                       verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 424,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_model = RandomForestClassifier(random_state=12340+41, n_estimators=41)\n",
    "final_model.fit(features_upsampled,target_upsampled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scaling test set\n",
    "features_test_edit = features_test.copy()\n",
    "features_test_edit.loc[:,numeric] = scaler.transform(features_test[numeric])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_test = final_model.predict(features_test_edit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6008119079837619"
      ]
     },
     "execution_count": 427,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(target_test, predicted_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8386495500232325"
      ]
     },
     "execution_count": 429,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probabilities_test = model.predict_proba(features_test_edit)\n",
    "probabilities_one_test = probabilities_test[:, 1]\n",
    "roc_auc_score(target_test, probabilities_one_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1635"
      ]
     },
     "execution_count": 428,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_test.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The data of 10000 entries is strongly skewed with 20% 1's\n",
    "- Test part is 20% of data, train is 80%\n",
    "- F1 score for constant model is 0.33 and AUC ROC is 0.5\n",
    "- Logistic Regression models didn't perform well with or without balancing and had bad 1's ratio of less than 10%\n",
    "- Random Forest models performed the best with upsampling with relatively large N=41\n",
    "- F1 score on test set is 0.6, AUC ROC is 0.83, 1's ratio is 16%"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
